# ğŸ“Š Exportation CSV RÃ©ussie !

## âœ… RÃ©sumÃ© de l'exportation

Le script `export_to_csv.py` a Ã©tÃ© crÃ©Ã© et exÃ©cutÃ© avec succÃ¨s. Il transforme la base de donnÃ©es SQL PostgreSQL (`data/events.sql`) en **10 fichiers CSV** sÃ©parÃ©s, prÃªts pour une utilisation avec AWS RAG.

### ğŸ“ Fichiers gÃ©nÃ©rÃ©s dans `csv_exports/`

| Fichier | Lignes | Description |
|---------|--------|-------------|
| **corrective_measure.csv** | 5,598 | Mesures correctives de sÃ©curitÃ© |
| **event.csv** | 2,359 | Ã‰vÃ©nements de sÃ©curitÃ©/incidents |
| **event_corrective_measure.csv** | 5,598 | Liaison Ã©vÃ©nements â†” mesures |
| **event_employee.csv** | 5,577 | Liaison Ã©vÃ©nements â†” employÃ©s |
| **event_risk.csv** | 6,372 | Liaison Ã©vÃ©nements â†” risques |
| **organizational_unit.csv** | 25 | UnitÃ©s organisationnelles |
| **person.csv** | 200 | Personnes/employÃ©s |
| **risk.csv** | 75 | Catalogue des risques |
| **measures_enriched.csv** | 5,598 | Vue enrichie des mesures (avec jointures) |
| **events_enriched.csv** | 15,358 | Vue enrichie des Ã©vÃ©nements (avec jointures) |

**Total : 25,804 lignes exportÃ©es**

## ğŸš€ Utilisation

### Pour rÃ©gÃ©nÃ©rer les fichiers CSV :

```powershell
python export_to_csv.py
```

### Structure du projet :

```
HackathonConformIT/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ events.sql          # Base de donnÃ©es source (PostgreSQL dump)
â”œâ”€â”€ csv_exports/            # Dossier des fichiers CSV gÃ©nÃ©rÃ©s
â”‚   â”œâ”€â”€ corrective_measure.csv
â”‚   â”œâ”€â”€ event.csv
â”‚   â”œâ”€â”€ events_enriched.csv # â­ Vue complÃ¨te pour RAG
â”‚   â””â”€â”€ ...
â”œâ”€â”€ export_to_csv.py        # âœ¨ Script principal d'export
â”œâ”€â”€ README_CSV.md           # Documentation dÃ©taillÃ©e
â””â”€â”€ events_complete.db      # Base SQLite temporaire
```

## ğŸ¯ Prochaines Ã©tapes pour AWS RAG

### 1. Upload vers S3

```python
import boto3

s3 = boto3.client('s3')
bucket_name = 'your-bucket-name'

# Upload tous les CSV
import os
for file in os.listdir('csv_exports'):
    if file.endswith('.csv'):
        s3.upload_file(
            f'csv_exports/{file}',
            bucket_name,
            f'rag-data/{file}'
        )
```

### 2. CrÃ©er une Knowledge Base dans Bedrock

1. AccÃ©dez Ã  Amazon Bedrock Console
2. CrÃ©ez une nouvelle Knowledge Base
3. Pointez vers votre bucket S3
4. Choisissez le modÃ¨le d'embedding (ex: Titan Embeddings)
5. Synchronisez les donnÃ©es

### 3. Fichiers recommandÃ©s pour RAG

Pour optimiser les performances RAG, utilisez principalement :

- **events_enriched.csv** : Vue complÃ¨te avec tous les dÃ©tails des Ã©vÃ©nements
- **measures_enriched.csv** : Mesures correctives avec contexte
- **event.csv** : Ã‰vÃ©nements bruts pour analyses dÃ©taillÃ©es

## ğŸ“ CaractÃ©ristiques des donnÃ©es

### Types d'Ã©vÃ©nements
- **EHS** : Environnement, HygiÃ¨ne et SÃ©curitÃ©
- **ENVIRONMENT** : Incidents environnementaux
- **DAMAGE** : Dommages matÃ©riels

### Classifications
- INJURY (Blessure)
- CHEMICAL_SPILL (DÃ©versement chimique)
- EQUIPMENT_FAILURE (DÃ©faillance Ã©quipement)
- NEAR_MISS (Quasi-accident)
- FIRE (Incendie)
- Et 8 autres classifications

### GravitÃ© des risques
- LOW (Faible)
- MEDIUM (Moyen)
- HIGH (Ã‰levÃ©)
- CRITICAL (Critique)

## ğŸ”§ Maintenance

### Mettre Ã  jour les donnÃ©es

1. Remplacez `data/events.sql` par la nouvelle version
2. ExÃ©cutez `python export_to_csv.py`
3. Les fichiers CSV seront automatiquement rÃ©gÃ©nÃ©rÃ©s

### Nettoyage

Pour supprimer les fichiers gÃ©nÃ©rÃ©s :

```powershell
Remove-Item csv_exports -Recurse -Force
Remove-Item events_complete.db -Force
```

## ğŸ“š Documentation

- **README_CSV.md** : Documentation complÃ¨te des structures de donnÃ©es
- **export_to_csv.py** : Code source commentÃ©

---

âœ¨ **Le fichier `csv.py` original a Ã©tÃ© corrigÃ© et remplacÃ© par `export_to_csv.py`** âœ¨

Le nouveau script :
- âœ… Lit directement depuis le fichier SQL dans `data/`
- âœ… CrÃ©e un fichier CSV par table (8 tables + 2 vues enrichies)
- âœ… GÃ¨re correctement les types de donnÃ©es et les jointures
- âœ… PrÃªt pour AWS RAG avec Amazon Bedrock
