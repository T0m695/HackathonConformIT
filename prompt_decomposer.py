"""
D√©composeur de prompts complexes avec AWS Bedrock
"""
import os
import json
import boto3
from dotenv import load_dotenv

load_dotenv()


def is_complex_prompt(prompt: str) -> bool:
    """D√©tecte si un prompt est complexe (multi-questions, multi-t√¢ches)"""
    indicators = [
        ' et ', ' puis ', ' ensuite ', ' √©galement ', ' aussi ',
        '?', 'combien', 'quels', 'comment', 'pourquoi',
        'compare', 'analyse', 'r√©sume', 'liste'
    ]
    
    prompt_lower = prompt.lower()
    complexity_score = sum(1 for ind in indicators if ind in prompt_lower)
    
    return complexity_score >= 3 or prompt.count('?') > 1 or len(prompt.split()) > 30


def convert_to_sql_query(prompt: str) -> str:
    """Convertit un prompt simple en requ√™te SQL via AWS Bedrock"""
    
    aws_config = {
        'region_name': os.getenv("AWS_DEFAULT_REGION", "us-east-1"),
        'aws_access_key_id': os.getenv("AWS_ACCESS_KEY_ID"),
        'aws_secret_access_key': os.getenv("AWS_SECRET_ACCESS_KEY"),
    }
    
    if token := os.getenv("AWS_SESSION_TOKEN"):
        aws_config['aws_session_token'] = token
    
    bedrock = boto3.client('bedrock-runtime', **aws_config)
    
    system_prompt = """Tu es un expert SQL. Convertis les questions en requ√™tes SQL PostgreSQL.
Tables disponibles: corrective_measure(measure_id, name, description, implementation_date, cost, organizational_unit_id), event(event_id, title, date, location), person(person_id, name, role), organizational_unit(unit_id, name), risk(risk_id, severity, description).
R√©ponds UNIQUEMENT avec la requ√™te SQL, sans explication."""
    
    user_message = f"""Question: {prompt}

G√©n√®re la requ√™te SQL PostgreSQL correspondante."""
    
    try:
        response = bedrock.invoke_model(
            modelId="anthropic.claude-3-haiku-20240307-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 200,
                "system": system_prompt,
                "messages": [{"role": "user", "content": user_message}],
                "temperature": 0.1
            })
        )
        
        response_body = json.loads(response['body'].read())
        sql_query = response_body['content'][0]['text'].strip()
        
        if 'SELECT' in sql_query.upper():
            sql_query = sql_query.replace('```sql', '').replace('```', '').strip()
            return sql_query
        
        return None
        
    except Exception as e:
        print(f"Erreur conversion SQL: {e}")
        return None


def decompose_prompt(prompt: str) -> list:
    """D√©compose un prompt complexe en sous-prompts simples via AWS Bedrock"""
    
    aws_config = {
        'region_name': os.getenv("AWS_DEFAULT_REGION", "us-east-1"),
        'aws_access_key_id': os.getenv("AWS_ACCESS_KEY_ID"),
        'aws_secret_access_key': os.getenv("AWS_SECRET_ACCESS_KEY"),
    }
    
    if token := os.getenv("AWS_SESSION_TOKEN"):
        aws_config['aws_session_token'] = token
    
    bedrock = boto3.client('bedrock-runtime', **aws_config)
    
    system_prompt = """Tu es un expert en d√©composition de questions.
D√©compose la question complexe en sous-questions simples et ind√©pendantes pour g√©n√©rer des requ√™tes SQL.
R√©ponds UNIQUEMENT avec une liste JSON de sous-questions, sans explication."""
    
    user_message = f"""Question complexe √† d√©composer:
"{prompt}"

Retourne un JSON avec cette structure:
{{"sub_prompts": ["question 1", "question 2", ...]}}"""
    
    try:
        response = bedrock.invoke_model(
            modelId="anthropic.claude-3-haiku-20240307-v1:0",
            body=json.dumps({
                "anthropic_version": "bedrock-2023-05-31",
                "max_tokens": 300,
                "system": system_prompt,
                "messages": [{"role": "user", "content": user_message}],
                "temperature": 0.3
            })
        )
        
        response_body = json.loads(response['body'].read())
        ai_text = response_body['content'][0]['text']
        
        # Extraire le JSON de la r√©ponse
        if '{' in ai_text and '}' in ai_text:
            json_start = ai_text.index('{')
            json_end = ai_text.rindex('}') + 1
            result = json.loads(ai_text[json_start:json_end])
            return result.get('sub_prompts', [prompt])
        
        return [prompt]
        
    except Exception as e:
        print(f"Erreur d√©composition: {e}")
        return [prompt]


def process_prompt(prompt: str):
    """Point d'entr√©e: d√©tecte complexit√© et retourne requ√™tes SQL ou sous-prompts"""
    
    if is_complex_prompt(prompt):
        print(f"üîç Prompt complexe d√©tect√©")
        sub_prompts = decompose_prompt(prompt)
        print(f"üìä D√©compos√© en {len(sub_prompts)} sous-prompts")
        
        sql_queries = []
        for sub_prompt in sub_prompts:
            sql = convert_to_sql_query(sub_prompt)
            if sql:
                sql_queries.append({"prompt": sub_prompt, "sql": sql})
        
        return sql_queries
    else:
        print(f"‚úÖ Prompt simple")
        sql = convert_to_sql_query(prompt)
        
        if sql:
            return [{"prompt": prompt, "sql": sql}]
        else:
            return [{"prompt": prompt, "sql": None}]


if __name__ == "__main__":
    # Test avec prompt simple
    simple = "Combien de mesures correctives en 2024?"
    print(f"Test 1: {simple}")
    result1 = process_prompt(simple)
    print(f"R√©sultat: {result1[0]['sql']}\n")
    
    # Test avec prompt complexe
    complex = "Compte les incidents de 2024, compare avec 2023, et liste les unit√©s avec le plus de mesures"
    print(f"Test 2: {complex}")
    result2 = process_prompt(complex)
    for i, r in enumerate(result2, 1):
        print(f"{i}. {r['prompt']}")
        print(f"   SQL: {r['sql']}\n")
